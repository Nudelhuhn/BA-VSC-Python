\chapter{Theoretische Grundlagen}

\section{Künstliche Intelligenz}
Künstliche Intelligenz (KI) kann vielfältig definiert werden. Im Zusammenhang dieser Arbeit bezeichnet sie den Teilbereich der Informatik, der sich mit der Entwicklung von Systemen befasst, die in der Lage sind, Aufgaben zu lösen, für die normalerweise menschliche Intelligenz erforderlich ist. Dazu gehören unter anderem das Erkennen von Mustern, das Treffen von Entscheidungen, das Verstehen natürlicher Sprache und das Lernen aus Erfahrungen (vgl. \cite{Bartneck.2021}). Der Mensch hat sich damit Systeme geschaffen, um die Kapazitäten menschlicher Intelligenz für bestimmte Aufgaben zu schonen oder zu erweitern. Dabei übernimmt Künstliche Intelligenz im Rahmen intelligenter Automatisierung Aufgaben, die zuvor von Menschen durchgeführt wurden, indem sie diese selbstständig und konsistent ausführt und dabei über die Zeit hinweg lernen, sich anpassen und verbessern kann, um Prozesse effizient und fehlerarm zu gestalten (vgl. \cite{Coombs.2020}). Während sich solche Systeme auf das Bildungswesen und hier speziell auf die Bewertung und Rückmeldung (Feedback) zu studentischen Programmierlösungen übertragen? Wie kann eine intelligente Automatisierung Lehrkräfte dabei unterstützen, qualitativ hochwertiges und individualisiertes Feedback zu generieren, ohne jede Lösung einzeln manuell prüfen zu müssen?

In dieser Arbeit wird KI anhand verschiedener Open-Source-Bibliotheken genutzt. Das System kombiniert mehrere KI-Techniken wie
\begin{itemize}
    \item Deep Learning - ein Teilbereich des Machine Learnings (ML), der künstliche neuronale Netze mit vielen Schichten verwendet, um komplexe Muster in Daten zu erkennen,
    \item Unsupervised Machine Learning - ein Verfahren, bei denen Modelle ohne beschriftete Trainingsdaten Muster oder Strukturen in den Daten erkennen, z.B. durch Clustering, und
    \item andere verschiedene Machine Learning Methoden, bei denen Computer aus Beispieldaten eigenständig Muster und Zusammenhänge erkennen, um daraus Vorhersagen oder Entscheidungen abzuleiten.
\end{itemize}

\section{Verwendete Algorithmen}

\subsection{Einbettung (Embedding)}
Einer der ersten Schritte des Programms ist das Einbetten von Quellcode-Text der Programmierlösungen, dessen Erstellung im späteren Verlauf der Arbeit erklärt wird. Der Quellcode-Text wird in numerische hochdimensionale Vektor-Repräsentationen (Embeddings) umgewandelt. Diese numerischen Vektoren fassen semantische und strukturelle Merkmale des Codes zusammen und machen die Daten für anschließende Verfahren wie Dimensionsreduktion, Clustering und Visualisierung nutzbar. Hier wurde eine erste Erwähnung solch eines Verfahrens in der Arbeit von Orvalho et al. (2022, \cite{Orvalho.28.06.2022}) erfasst. Das dort beschriebene Verfahren CodeBERT stellte sich durch weitere Recherche für diese Arbeit als geeignet heraus. CodeBERT ist ein auf die Transformer-Architektur\footnote{Neuronales Netzwerkmodell, das mithilfe von Self-Attention-Mechanismen die Beziehungen zwischen Elementen in einer Sequenz erfasst und dadurch besonders leistungsfähig für Aufgaben mit Text- oder Code-Daten ist} basiertes Modell, das gleichzeitig mit natürlicher Sprache und Programmiercode (u.a. Java und Python) trainiert wurde. Es verwendet dabei machine learning (ML) wie Masked Language Modeling\footnote{Trainingsmethode, bei der zufällig Wörter im Text verdeckt werden und das Modell lernen soll, diese fehlenden Wörter richtig vorherzusagen} und Replaced Token Detection\footnote{Trainingsmethode, bei der das Modell lernt zu erkennen, welche Wörter im Text durch andere ersetzt wurden, um so bessere Sprachrepräsentationen zu entwickeln.}, um inhaltlich sinnvolle und strukturierte Vektorrepräsentationen (Embeddings) für Code zu erzeugen (vgl. \cite{Feng.19.02.2020}).

\subsection{Dimensionsreduktion}
Weiterhin wurden Verfahren eingebunden, die die durch das Embedding erstellten hochdimensionale Vektoren in ihren Dimensionen reduzieren, um sie ebenso für weiterführende Prozesse wie z. B. zur Clusterung und besonders zur Visualisierung in den zwei- oder dreidimensionalen Raum nutzbar zu machen. In dieser Arbeit wurden folgende Verfahren benutzt:
\begin{itemize}
    \item Principal Component Analysis (PCA) - projiziert hochdimensionale Daten in einen lineareren Unterraum mit geringerer Dimension, indem neue Achsen, entlang derer die Daten am stärksten streuen, berechnet werden und stellt die Daten entlang dieser Achsen dar (vgl. \cite{KarlPearson.1901}),
    \item t-Distributed Stochastic Neighbor Embedding (t-SNE) - visualisiert hochdimensionaler Daten, indem es berechnet wie ähnlich Punkte zu ihren Originaldaten sind, um sie entsprechend weit auseinander oder nahe zusammen zu platzieren (vgl. \cite{LaurensvanderMaatenundGeoffreyHinton.2008}), und
    \item Uniform Manifold Approximation and Projection (UMAP) - nutzt Topologie und Geometrie, um skalierbare, strukturtreue Einbettungen in niedrigere Dimensionen zu erreichen (vgl. \cite{McInnes.09.02.2018}).
\end{itemize}
Diese Algorithmen wurden bevorzugt, da sie aufgrund ihrer Verfügbarkeit in öffentlichen Bibliotheken in das vorgestellte Python Projekt einfach eingebunden werden konnten.

\subsection{Gruppierung (Clustering)}
Das zentral angesprochene Verfahren ist das Clustering. Inspiration für diese Arbeit wurde aus aktuellen Werken entnommen, wie Orvalho et al. (2022), die mit InvAASTCluster ein Verfahren zur Clusterung von Programmierlösungen mittels dynamischer Invarianten-Analyse vorstellen (vgl. \cite{Orvalho.28.06.2022}); aus Paiva et al. (2024) die AsanasCluster, ein inkrementelles k-Means-basiertes Verfahren, zur Clusterung von Programmierlösungen für automatisiertes Feedback entwickelt haben (vgl. \cite{Paiva.2024}); und Tang et al. (2024) die Large Language Models\footnote{auf Textdaten trainierte KI-Modelle, die natürliche Sprache verarbeiten und generieren} (LLMs) und Clustering kombinieren, um personalisiertes Feedback in Programmierkursen zu skalieren (vgl. \cite{Tang.21.10.2024}).

Die Algorithmen dieser Quellen und weitere Recherche dienten zum Kennenlernen und zum späteren Einbinden von Algorithmen, die aufgrund ihrer besonderen Eignung zur Clusterung von Programmieraufgaben herausstachen wie 
\begin{itemize}
    \item k-Means - teilt N Beobachtungen in k Cluster auf, wobei jede Beobachtung zu dem Cluster mit dem nächstgelegenen Mittelwert gehört, der als Prototyp des Clusters dient (vgl. \cite{MacQueen.1967}), und
    \item HDBSCAN - erweitert des Density-Based Spatial Clustering of Applications with Noise (DBSCAN) Algorithmus, indem es eine Hierarchie von Clustern aufbaut und die stabilen Cluster über unterschiedliche Dichteebenen hinweg extrahiert (vgl. \cite{CampelloRicardoJ.G.B..}),
\end{itemize}

\subsection{Visualisierung}
Zur Visualisierung der zuvor geclusterten studentischen Programmierlösungen wurden die Python-Bibliotheken pandas und plotly.express verwendet:
\begin{itemize}
    \item pandas: Dient zur effizienten Verarbeitung und Analyse von tabellarischen Daten. In diesem Fall wurden damit die Cluster-Zuordnungen und die zugehörigen Punktkoordinaten in einer DataFrame-Struktur verwaltet.
    \item plotly.express: Eine High-Level-Bibliothek für interaktive Diagramme. Sie wurde genutzt, um die studentischen Lösungen als farbige Punkte in einem Streudiagramm darzustellen, wobei die Farbe jeweils das zugehörige Cluster repräsentiert.
\end{itemize}
So konnte die Qualität und Trennschärfe der Clusterung visuell überprüft werden.

\subsection{Evaluierung}
Das Projekt wurde so erstellt, dass für ein beliebigen Clustering-Algorithmus ein Diagramm erstellt wird, in denen farbige Punkte die entsprechenden studentischen Lösungen repräsentieren. Um diesen Prozess zu bewerten wurden Evaluierungsverfahren etabliert. Nach Halkidi et al. 2001 bewerten interne Clustering-Evaluierungsverfahren die Qualität einer Clusterlösung anhand der Dichte innerhalb der Cluster und der Trennung zwischen den Clustern, ohne dabei externe Referenzdaten heranzuziehen (vgl. \cite{Halkidi.2001}). In dieser Arbeit wurden erstmalig Erwähnungen solcher Verfahren in \cite{YoussefLahmadiMohammedZakariaeElKhattabiMouniaRahhaliLahcenOughdir.2024} entdeckt, wobei daraus nur zwei der benutzen Verfahren und erst durch weitere Recherche ein drittes hier eingebunden wurde. Folgende Auflistung beschreibt die benutzten Verfahren:
\begin{itemize}
    \item Silhouette Score - berechnet für jeden Datenpunkt einen Silhouette-Wert, der die Qualität der Clusterzuordnung anhand der Abstände innerhalb und zwischen Clustern bewertet (vgl. \cite{Rousseeuw.1987})
    \item Caliński-Harabasz Index - bewertet die Clusterqualität anhand des Verhältnisses von Streuung zwischen und innerhalb der Cluster. Das Verfahren wurde ursprünglich von Calinski und Harabasz (1974,  \cite{CalinskiT.andHarabaszJ..1974}) eingefügt, eine Beschreibung findet sich in \cite{Halkidi.2001}.
    \item Davies-Bouldin Index - bewertet die Clusterqualität anhand des Verhältnisses von Intra-Cluster-Distanzen zu den Distanzen zwischen den Clustermittelpunkten. Das Verfahren wurde ursprünglich von Davies und Bouldin (1979, \cite{Davies.1979}) eingeführt, eine Beschreibung findet sich beispielsweise in der scikit-learn-Dokumentation\footnote{\url{https://scikit-learn.org/stable/modules/clustering.html\#clustering-performance-evaluation}}.
\end{itemize}